{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polarice\n",
    "import gensim.downloader as api\n",
    "\n",
    "dataset = polarice.PolarizationDataset.load(\"data/dataset.p\")\n",
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrameAxis \n",
    "\n",
    "This module is built upon the FrameAxis approach [^1].\n",
    "\n",
    "[^1] Kwak, H., An, J., & Ahn, Y. Y. (2020). FrameAxis: Characterizing Framing Bias and Intensity with Word Embedding. arXiv preprint arXiv:2002.08608.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frame_axis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frame_axis.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "import pickle as pkl\n",
    "from collections import namedtuple, OrderedDict\n",
    "import tqdm\n",
    "\n",
    "\"\"\"Hierarchical representations for frame analysis: FrameWord --> FramePoles --> FrameAxes --> FrameSystem.\"\"\"\n",
    "\n",
    "class FrameWord:\n",
    "    \"\"\"Represents a single framing word. Mainly a wrapper.\"\"\"\n",
    "    \n",
    "    def __init__(self, word, model=None):\n",
    "        self.word = word\n",
    "        self.model = model\n",
    "\n",
    "class FramePole:\n",
    "    \"\"\"Represents a pole of the FrameAxis. Thus either positive or negative words.\"\"\"\n",
    "    \n",
    "    def __init__(self, pole_name, words, model):\n",
    "        self.pole_name = pole_name\n",
    "        self.words = words\n",
    "        self.initial_words = words  # For debugging\n",
    "        self.model = model\n",
    "        \n",
    "    def compute(self):\n",
    "        \"\"\"Computes everything for its usage (e.g., centroid).\"\"\"\n",
    "        self.retain_model_words_only()\n",
    "        self.compute_centroid()\n",
    "        return self\n",
    "        \n",
    "    def retain_model_words_only(self, log_removed=False):\n",
    "        \"\"\"Cleans the initial words to fit the supplied model.\"\"\"\n",
    "        pole_words = []\n",
    "        for pole_word in self.words:\n",
    "            if pole_word in self.model.key_to_index:\n",
    "                pole_words.append(pole_word)\n",
    "            else:\n",
    "                if log_removed:\n",
    "                    print(f\"Word {pole_word} not in vocab\")\n",
    "        self.words = pole_words\n",
    "        return pole_words\n",
    "    \n",
    "    def extract_vectors_from_model(self):\n",
    "        \"\"\"Extract the relevant vectors from the model. In same order as the words.\"\"\"\n",
    "        pole_vecs = []\n",
    "        for pole_word in self.words:\n",
    "            vec = self.model.get_vector(pole_word)\n",
    "            pole_vecs.append(vec)\n",
    "        print(len(pole_vecs))\n",
    "        self.pole_vecs = pole_vecs\n",
    "        return pole_vecs\n",
    "        \n",
    "    def compute_centroid(self):\n",
    "        \"\"\"Computes the centroid and vectors. Assumes valid vocabulary. Call `retain_model_words_only` beforehand.\"\"\"\n",
    "        pole_vecs = self.extract_vectors_from_model()\n",
    "        centroid = np.mean(pole_vecs, axis=0)\n",
    "        self.centroid = centroid\n",
    "        return centroid\n",
    "\n",
    "class FrameAxis:\n",
    "    \"\"\"Represents a Frame Axis, which is a Semantic Axis (SemAxis) with Bias and Intensity.\"\"\"\n",
    "    \n",
    "    def __init__(self, name, pos_words, neg_words, wv_name, word_vectors):\n",
    "        self.name = name\n",
    "        self.pos_words = pos_words\n",
    "        self.neg_words = neg_words\n",
    "        self.wv_name = wv_name  # required to reproduce frame_axis\n",
    "        self.word_vectors = word_vectors\n",
    "        # TODO: compute axis\n",
    "        self.axis = None\n",
    "        self.baseline_bias = None\n",
    "        self.model = word_vectors\n",
    "        \n",
    "        self.sim_cache = dict()  # use it to cache word similarities for reuse\n",
    "    \n",
    "    @classmethod\n",
    "    def from_poles(cls, pos_pole, neg_pole):\n",
    "        name = pos_pole.pole_name + \"/\" + neg_pole.pole_name\n",
    "        assert pos_pole.model == neg_pole.model\n",
    "        return cls(name, pos_pole.words, neg_pole.words, \"\", pos_pole.model)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pkl.load(f)\n",
    "        \n",
    "    def save(self, filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pkl.dump(self, f)\n",
    "        \n",
    "    def attach_model(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def detach_model(self):\n",
    "        self.model = None\n",
    "        \n",
    "    def compute_word_sim(self, word):\n",
    "        pass\n",
    "    \n",
    "    def compute(self):\n",
    "        self.pos_words, _ = self.retain_words_in_model(self.pos_words, self.model)\n",
    "        self.neg_words, _ = self.retain_words_in_model(self.neg_words, self.model)\n",
    "        self.compute_axis()\n",
    "        return self\n",
    "        \n",
    "    def compute_axis(self):\n",
    "        pos_centroid, pos_vecs = self.compute_centroid(self.pos_words, self.word_vectors)\n",
    "        neg_centroid, neg_vecs = self.compute_centroid(self.neg_words, self.word_vectors)\n",
    "        self.pos_centroid = pos_centroid\n",
    "        self.neg_centroid = neg_centroid\n",
    "        axis = pos_centroid - neg_centroid\n",
    "        self.axis = axis\n",
    "        return axis\n",
    "        \n",
    "    def retain_words_in_model(self, initial_words, model=None):\n",
    "        if not model:\n",
    "            model = self.model\n",
    "        \n",
    "        words_in_vocab = []\n",
    "        words_not_in_vocab = []\n",
    "        for word in initial_words:\n",
    "            if word in model.key_to_index:\n",
    "                words_in_vocab.append(word)\n",
    "            else:\n",
    "                words_not_in_vocab.append(word)\n",
    "        return words_in_vocab, words_not_in_vocab\n",
    "\n",
    "    def compute_centroid(self, frame_words, model=None):\n",
    "        \"\"\"Assumes valid list of words.\"\"\"\n",
    "        if not model:\n",
    "            model = self.model\n",
    "        \n",
    "        frame_vecs = []\n",
    "        for frame_word in frame_words:\n",
    "            assert frame_word in model.key_to_index\n",
    "            vec = model.get_vector(frame_word)\n",
    "            frame_vecs.append(vec)\n",
    "        centroid = np.mean(frame_vecs, axis=0)\n",
    "        return centroid, frame_vecs\n",
    "    \n",
    "    def compute_bias_document(self, doc, model=None):\n",
    "        if not model:\n",
    "            model = self.model\n",
    "    \n",
    "        word_vecs = []\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            if not word in model.key_to_index:\n",
    "                continue\n",
    "            word_vec = model.get_vector(word)\n",
    "            word_vecs.append(word_vec)\n",
    "        if not word_vecs:\n",
    "            return 0  # No bias when no words\n",
    "        sims = model.cosine_similarities(self.axis, word_vecs)\n",
    "        return np.sum(sims) / len(words)\n",
    "        \n",
    "    def compute_baseline_bias(self, docs, model=None):\n",
    "        if not model:\n",
    "            model = self.model\n",
    "        \n",
    "        doc_biases = []\n",
    "        for doc in docs:\n",
    "            doc_biases.append(self.compute_bias_document(doc, model))\n",
    "        baseline_bias = np.mean(doc_biases)\n",
    "        self.baseline_bias = baseline_bias\n",
    "        return baseline_bias\n",
    "    \n",
    "    def compute_intensity_document(self, doc, baseline_bias=None):\n",
    "        if not baseline_bias:\n",
    "            baseline_bias = self.baseline_bias\n",
    "        if not baseline_bias:\n",
    "            raise ValueError(\"Neithher baseline_bias provided nor inherent to object.\")\n",
    "        word_vecs = []\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            if not word in self.word_vectors.key_to_index:\n",
    "                continue\n",
    "            word_vec = self.word_vectors.get_vector(word)\n",
    "            word_vecs.append(word_vec)\n",
    "        if not word_vecs:\n",
    "            return 0  # No bias when no words\n",
    "        sims = self.word_vectors.cosine_similarities(self.axis, word_vecs)\n",
    "        sim_dev = (sims - baseline_bias)**2\n",
    "        return np.sum(sim_dev) / len(words)\n",
    "    \n",
    "    def compute_baseline_intensity(self, docs, baseline_bias=None, model=None):\n",
    "        if not model:\n",
    "            model = self.model\n",
    "        \n",
    "        doc_intensities = []\n",
    "        for doc in docs:\n",
    "            doc_intensity = self.compute_intensity_document(doc, baseline_bias)\n",
    "            doc_intensities.append(doc_intensity)\n",
    "        baseline_intensity = np.mean(doc_intensities)\n",
    "        self.baseline_intensity = baseline_intensity\n",
    "        return baseline_intensity\n",
    "\n",
    "    def effect_size(self, corpus: pd.Series, num_bootstrap_samples=1000):\n",
    "        corpus_bias = self.compute_baseline_bias(corpus)\n",
    "        corpus_intensity = self.compute_baseline_intensity(corpus, baseline_bias=corpus_bias)\n",
    "        boostrap_samples = [corpus.sample(n=len(corpus), replace=True) for _ in range(num_bootstrap_samples)]\n",
    "        \n",
    "        cum_sample_bias = 0\n",
    "        cum_sample_intensity = 0\n",
    "        for sample in tqdm.tqdm(boostrap_samples):\n",
    "            sample = corpus.sample(n=len(corpus))\n",
    "            bootstrapped_bias = self.compute_baseline_bias(sample)\n",
    "            bootstrapped_intensity = self.compute_baseline_intensity(sample, baseline_bias=bootstrapped_bias)\n",
    "            cum_sample_bias += bootstrapped_bias\n",
    "            cum_sample_intensity += bootstrapped_intensity\n",
    "        # Effect sizes for bias and intensity\n",
    "        eta_bias = abs(corpus_bias - cum_sample_bias/num_bootstrap_samples)\n",
    "        eta_intensity = abs(corpus_intensity - cum_sample_intensity/num_bootstrap_samples)\n",
    "        \n",
    "        EffectSize = namedtuple(\"EffectSize\", [\"eta_bias\", \"eta_intensity\"])\n",
    "        return EffectSize(eta_bias, eta_intensity)\n",
    "\n",
    "def compute_bias(document, frame_vector, model):\n",
    "    word_vecs = []\n",
    "    words = document.split(\" \")\n",
    "    for word in words:\n",
    "        if not word in model.key_to_index:\n",
    "            continue\n",
    "        word_vec = model.get_vector(word)\n",
    "        word_vecs.append(word_vec)\n",
    "    if not word_vecs:\n",
    "        return 0  # No bias when no words\n",
    "    sims = model.cosine_similarities(frame_vector, word_vecs)\n",
    "    return np.sum(sims) / len(words)\n",
    "\n",
    "def compute_intensity(document, frame_vector, model, baseline_bias):\n",
    "    word_vecs = []\n",
    "    words = document.split(\" \")\n",
    "    for word in words:\n",
    "        if not word in model.key_to_index:\n",
    "            continue\n",
    "        word_vec = model.get_vector(word)\n",
    "        word_vecs.append(word_vec)\n",
    "    if not word_vecs:\n",
    "        return 0  # No bias when no words\n",
    "    sims = model.cosine_similarities(frame_vector, word_vecs)\n",
    "    sim_dev = (sims - baseline_bias)**2\n",
    "    return np.sum(sim_dev) / len(words)\n",
    "\n",
    "class FrameSystem:\n",
    "    \"\"\"Represents a set of FrameAxes that form a complete system.\"\"\"\n",
    "    \n",
    "    def __init__(self, frame_axes: Dict[str, FrameAxis]):\n",
    "        self.frame_axes = frame_axes\n",
    "        \n",
    "    def transform_df(self, df: pd.DataFrame, text_col: str, model):\n",
    "        for name, axis in self.frame_axes.items():\n",
    "            pos_name, neg_name = name.split(\"/\")  # TODO: move this informtion to FrameAxis\n",
    "            axis_code = pos_name[:4]\n",
    "            df[axis_code + \"_bias\"] = df[text_col].map(lambda x: compute_bias(x, axis.axis, model))\n",
    "            baseline_bias = df[axis_code + \"_bias\"].mean()\n",
    "            df[axis_code + \"_inte\"] = df[text_col].map(lambda x: compute_intensity(x, axis.axis, model, baseline_bias))\n",
    "        return df\n",
    "    \n",
    "    def axes_ordered_by_effect_sizes(self, corpus: pd.Series, num_bootstrap_samples=1000, sort_key=\"eta_bias\"):\n",
    "        axes_effect_sizes = {}\n",
    "        for name, axis in self.frame_axes.items():\n",
    "            axes_effect_sizes[name] = axis.effect_size(corpus=corpus, num_bootstrap_samples=num_bootstrap_samples)\n",
    "        return OrderedDict(sorted(axes_effect_sizes.items(), key=lambda x: -getattr(x[1], sort_key)))\n",
    "    \n",
    "    def compute_baseline_biases(self, df: pd.DataFrame, text_col: str, model):\n",
    "        baseline_biases = {}\n",
    "        for name, axis in self.frame_axes.items():\n",
    "            pos_name, neg_name = name.split(\"/\")  # TODO: move this informtion to FrameAxis\n",
    "            axis_code = pos_name[:4]\n",
    "            baseline_bias = df[text_col].map(lambda x: compute_bias(x, axis.axis, model)).mean()\n",
    "            baseline_biases[name] = baseline_bias\n",
    "        return baseline_biases\n",
    "    \n",
    "    def attach_model(self, model):\n",
    "        self.model = model\n",
    "        for name, axis in self.frame_axes.items():\n",
    "            axis.attach_model(model)\n",
    "            \n",
    "    def compute(self):\n",
    "        for name, axis in self.frame_axes.items():\n",
    "            axis.compute()\n",
    "        return self\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pkl.load(f)\n",
    "        \n",
    "    def save(self, filename):\n",
    "        # Detach models before storing\n",
    "        for _, frame_axis in self.frame_axes.items():\n",
    "            frame_axis.detach_model()\n",
    "        \n",
    "        with open(filename, \"wb\") as f:\n",
    "            pkl.dump(self, f)\n",
    "            \n",
    "# Helper classes\n",
    "            \n",
    "class FrameExperiment:\n",
    "    \"\"\"Combination of Dictionary Mapping, WordEmbeddings, and Dataset. Used to conduct evaluations.\"\"\"\n",
    "    def __init__(self, dictionary_mapping, wordembeddings, dataset):\n",
    "        self.dictionary_mapping = dictionary_mapping\n",
    "        self.wordembeddings = wordembeddings\n",
    "        self.dataset = dataset\n",
    "        \n",
    "class FrameVisualization:\n",
    "    \"\"\"Used to visualize the frame system.\"\"\"\n",
    "    def __init__(self, frame_system: FrameSystem):\n",
    "        self.frame_system = frame_system\n",
    "            \n",
    "def frame_axis_polarization(frame_axis):\n",
    "    pos_centroid = frame_axis.pos_centroid\n",
    "    neg_centroid = frame_axis.neg_centroid\n",
    "    pos = frame_axis.pos_words\n",
    "    neg = frame_axis.neg_words\n",
    "    \n",
    "    word_vectors = frame_axis.word_vectors\n",
    "    \n",
    "    inter_dist_pos = np.mean(word_vectors.distances(pos_centroid, pos))\n",
    "    inter_dist_neg = np.mean(word_vectors.distances(neg_centroid, neg))\n",
    "    between_dist_pos = np.mean(word_vectors.distances(pos_centroid, neg))\n",
    "    between_dist_neg = np.mean(word_vectors.distances(neg_centroid, pos))\n",
    "    print(f\"{inter_dist_pos=} {inter_dist_neg=} {between_dist_pos=} {between_dist_neg=}\")\n",
    "    \n",
    "    return (between_dist_pos+between_dist_neg)/2-(inter_dist_pos+inter_dist_neg)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run frame_axis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03649503,  0.44649416,  1.3033249 , -0.09901801, -0.543165  ,\n",
       "        0.06154996,  1.1722801 , -0.11289489,  0.37668502,  0.62156   ,\n",
       "       -0.52041996,  0.10456613, -0.072725  , -0.25025004, -0.2866685 ,\n",
       "        0.774215  ,  1.00493   ,  0.42712498,  0.28918886, -1.32272   ,\n",
       "        0.46895504, -0.16249001,  0.58235496, -1.5769    , -1.02247   ,\n",
       "       -0.10630002,  0.30814502,  0.64058   ,  0.317215  , -0.03951496,\n",
       "        0.29331493, -0.2059555 , -0.30391002, -0.70380497,  0.903245  ,\n",
       "        0.24992001,  0.746135  ,  0.08126003,  1.6797299 , -0.38700998,\n",
       "       -0.47811   , -0.5595455 ,  1.02496   ,  0.20258254,  0.3488285 ,\n",
       "        0.51743495, -0.50139296,  0.410907  ,  0.56883854,  0.340655  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pole1 = [\"carbon\", \"mitigation\"]\n",
    "pole2 = [\"migration\", \"adaption\"]\n",
    "fa = FrameAxis(\"Climate\", pole1, pole2, \"glove-wiki-gigaword-50\", model)\n",
    "fa.compute_axis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5866973996162415\n"
     ]
    }
   ],
   "source": [
    "bias_pole1 = fa.compute_bias_document(\" \".join(pole1), model)\n",
    "print(bias_pole1)\n",
    "assert bias_pole1 > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3108203411102295\n"
     ]
    }
   ],
   "source": [
    "bias_pole2 = fa.compute_bias_document(\" \".join(pole2), model)\n",
    "print(bias_pole2)\n",
    "assert bias_pole2 < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From here on, we need a model\n",
    "fa.attach_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1100.000000\n",
       "mean        0.091483\n",
       "std         0.014653\n",
       "min         0.048531\n",
       "25%         0.079724\n",
       "50%         0.089863\n",
       "75%         0.102914\n",
       "max         0.130834\n",
       "Name: bias, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df[\"bias\"] = dataset.df[dataset.text_column].map(fa.compute_bias_document)\n",
    "dataset.df[\"bias\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    500.000000\n",
       "mean       0.079753\n",
       "std        0.008566\n",
       "min        0.048531\n",
       "25%        0.074649\n",
       "50%        0.079393\n",
       "75%        0.084895\n",
       "max        0.105505\n",
       "Name: bias, dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df[dataset.df[\"group\"] == \"adaption\"][\"bias\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    500.000000\n",
       "mean       0.101707\n",
       "std        0.011240\n",
       "min        0.071160\n",
       "25%        0.094788\n",
       "50%        0.102441\n",
       "75%        0.109304\n",
       "max        0.130834\n",
       "Name: bias, dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df[dataset.df[\"group\"] == \"mitigation\"][\"bias\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09148284338588908"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa.compute_baseline_bias(dataset.df[dataset.text_column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1100.000000\n",
       "mean        0.014131\n",
       "std         0.003604\n",
       "min         0.007037\n",
       "25%         0.011049\n",
       "50%         0.013707\n",
       "75%         0.017065\n",
       "max         0.025742\n",
       "Name: inte, dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df[\"inte\"] = dataset.df[dataset.text_column].map(fa.compute_intensity_document)\n",
    "dataset.df[\"inte\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.FrameAxis at 0x7f18d785a070>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test IO\n",
    "import tempfile\n",
    "\n",
    "tempdir = tempfile.gettempdir() + \"/\"\n",
    "fa.save(tempdir + \"test.pkl\")\n",
    "FrameAxis.load(tempdir + \"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inter_dist_pos=0.16403705 inter_dist_neg=0.23015118 between_dist_pos=0.7200742 between_dist_neg=0.68450624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5051960647106171"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_axis_polarization(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EffectSize(eta_bias=1.5796836061454655e-11, eta_intensity=3.761895309151164e-12)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa.effect_size(dataset.df[dataset.text_column], num_bootstrap_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polarice",
   "language": "python",
   "name": "polarice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
